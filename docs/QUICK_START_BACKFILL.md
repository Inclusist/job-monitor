# Quick Start: Backfill System

Complete setup guide for the user backfill and daily job loading system.

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Job Monitor System                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  User Signup â”‚ â”€â”€â”€â”€â”€â”€> â”‚  Backfill    â”‚ (1 month)          â”‚
â”‚  â”‚  + CV Upload â”‚         â”‚  Service     â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                   â”‚                             â”‚
â”‚                                   â–¼                             â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                          â”‚  PostgreSQL DB   â”‚                  â”‚
â”‚                          â”‚  â€¢ Jobs          â”‚                  â”‚
â”‚                          â”‚  â€¢ User Queries  â”‚                  â”‚
â”‚                          â”‚  â€¢ Tracking      â”‚                  â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                   â–²                             â”‚
â”‚                                   â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Daily Cron  â”‚ â”€â”€â”€â”€â”€â”€> â”‚  Daily Job   â”‚ (24 hours)         â”‚
â”‚  â”‚  (6 AM CEST) â”‚         â”‚  Loader      â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Prerequisites

1. **Database:** PostgreSQL configured
2. **API Keys:**
   - JSearch API key (RapidAPI)
   - Active Jobs DB API key (RapidAPI)
   - Anthropic API key (for CV parsing)
3. **Environment:** Python 3.13+ with venv

---

## Step 1: Install Dependencies

```bash
# Activate virtual environment
source venv/bin/activate

# Install all dependencies (including APScheduler)
pip install -r requirements.txt
```

---

## Step 2: Run Database Migrations

### Migration 1: Normalize User Queries

Creates normalized `user_search_queries` table (titleÃ—location as separate rows):

```bash
python scripts/migrate_normalize_user_queries.py
```

**Expected output:**
```
MIGRATION: Normalize user_search_queries table
âœ“ Table already exists, columns already added
âœ… Migration completed successfully!
```

### Migration 2: Add Backfill Tracking

Creates `backfill_tracking` table to track which combinations are backfilled:

```bash
python scripts/migrate_add_backfill_tracking.py
```

**Expected output:**
```
MIGRATION: Add backfill tracking table
ğŸ“ Creating backfill_tracking table...
âœ… Migration completed successfully!
```

---

## Step 3: Configure Environment Variables

Ensure `.env` file has:

```bash
# Database
DATABASE_URL=postgresql://user:password@localhost/job_monitor

# API Keys
ANTHROPIC_API_KEY=sk-ant-...
JSEARCH_API_KEY=...
ACTIVEJOBS_API_KEY=...

# Flask
SECRET_KEY=<random-string>
FLASK_ENV=development
```

---

## Step 4: Test Backfill System (Optional)

Test with two users to verify deduplication works:

```bash
python scripts/test_backfill_flow.py
```

**What it does:**
1. Creates User 1 with search queries
2. Runs backfill for User 1 (fetches 1 month of jobs)
3. Creates User 2 with overlapping queries
4. Runs backfill for User 2 (skips already-backfilled combinations!)

**Expected output:**
```
TEST 1: Create User 1 with Unique Queries
âœ“ Created 4 query rows (2 titles Ã— 2 locations = 4 rows)
âœ“ User 1 has 4 unbacked combinations

TEST 2: Backfill User 1
âœ“ Backfill completed for User 1:
   - Jobs added: 150

TEST 3: Create User 2 with Overlapping Queries
âœ“ Created 2 query rows (1 title Ã— 2 locations = 2 rows)
âœ“ User 2 has 1 unbacked combinations  <-- Deduplication works!
   âœ… DEDUPLICATION WORKS!
      Expected 1 unbacked, got 1
      'Data Scientist in Berlin' already backfilled by User 1
```

---

## Step 5: Verify CV Upload Integration

The backfill is automatically triggered when users upload CVs.

**Test it:**

1. Start Flask app:
```bash
python app.py
```

2. Upload a CV via web UI or API:
```bash
curl -X POST http://localhost:8080/upload-cv \
  -F "cv=@test_cv.pdf" \
  -F "user_email=test@example.com"
```

3. Check logs for backfill trigger:
```
âœ“ Auto-generated 4 search query rows for test@example.com
  Combinations: 2 titles Ã— 2 locations = 4 rows

ğŸ”„ Triggering 1-month backfill for test@example.com...
âœ“ Backfill completed: 150 jobs added
```

---

## Step 6: Set Up Daily Cron Job

### Option A: Local Testing (Run Once)

```bash
python scripts/daily_job_cron.py --run-once
```

**Output:**
```
DAILY JOB STARTED - 2024-01-15 14:30:00 CET
ğŸ“¥ Loading jobs from last 24 hours...

Query Analysis:
  Total query rows: 8
  Unique combinations: 5
  âœ… Quota saved: 3 API calls (37.5%)

âœ… Daily job completed successfully!
   â€¢ Unique combinations: 5
   â€¢ Jobs fetched: 450
   â€¢ New jobs added: 380
```

### Option B: Run as Cron Service (Continuous)

```bash
python scripts/daily_job_cron.py --schedule "6:00"
```

**Output:**
```
DAILY JOB CRON SERVICE
Schedule: Every day at 6:00 CEST
Started: 2024-01-15 14:30:00

âœ“ Scheduled daily job at 6:00 CEST
  Next run: 2024-01-16 06:00:00+01:00

Press Ctrl+C to stop the service
```

---

## Step 7: Deploy to Railway

See [RAILWAY_DEPLOYMENT.md](./RAILWAY_DEPLOYMENT.md) for complete deployment guide.

**Quick summary:**

1. **Create Railway project** â†’ Connect GitHub repo
2. **Deploy web service:**
   - Start: `gunicorn app:app --bind 0.0.0.0:$PORT`
   - Env: All API keys + `DATABASE_URL`
3. **Deploy cron service:**
   - Start: `python scripts/daily_job_cron.py --schedule "6:00"`
   - Env: `ACTIVEJOBS_API_KEY`, `DATABASE_URL`, `TZ=Europe/Berlin`
4. **Add PostgreSQL database** â†’ Auto-connects to both services
5. **Run migrations** (via web service build command)

---

## How It Works

### User Signup Flow

```
User uploads CV
    â†“
Claude parses CV â†’ Extracts desired titles, locations, preferences
    â†“
Queries auto-generated â†’ Stored as normalized rows
    â†“
Check backfill_tracking â†’ Which combinations are NEW?
    â†“
Backfill NEW combinations only â†’ Fetch 1 month of jobs
    â†“
Mark as backfilled â†’ Future users with same queries skip this!
```

### Daily Updates Flow

```
Cron triggers at 6 AM CEST
    â†“
Get unique combinations across ALL users (SELECT DISTINCT)
    â†“
Fetch last 24 hours for each unique combination
    â†“
Deduplicate and store jobs
    â†“
All users see fresh jobs
```

---

## Key Files

| File | Purpose |
|------|---------|
| `src/jobs/user_backfill.py` | Backfill service (1 month) |
| `scripts/user_query_loader.py` | Daily job loader (24h) |
| `scripts/daily_job_cron.py` | Cron scheduler |
| `src/cv/cv_handler.py` | CV upload + backfill trigger |
| `src/database/postgres_cv_operations.py` | DB methods for queries & tracking |
| `scripts/migrate_normalize_user_queries.py` | Migration 1 |
| `scripts/migrate_add_backfill_tracking.py` | Migration 2 |
| `scripts/test_backfill_flow.py` | Test script |

---

## Database Tables

### user_search_queries
Stores normalized query combinations per user

```sql
SELECT * FROM user_search_queries WHERE user_id = 1;

id | user_id | title_keyword      | location | ai_work_arrangement
1  | 1       | Data Scientist     | Berlin   | Remote OK
2  | 1       | Data Scientist     | Hamburg  | Remote OK
3  | 1       | ML Engineer        | Berlin   | Remote OK
4  | 1       | ML Engineer        | Hamburg  | Remote OK
```

### backfill_tracking
Tracks which combinations are backfilled globally

```sql
SELECT * FROM backfill_tracking;

id | title_keyword      | location | backfilled_date      | jobs_found
1  | Data Scientist     | Berlin   | 2024-01-15 10:30:00 | 45
2  | Data Scientist     | Hamburg  | 2024-01-15 10:35:00 | 32
3  | ML Engineer        | Berlin   | 2024-01-15 10:40:00 | 28
```

---

## Monitoring

### Check Quota Usage

Daily job prints quota analysis:
```
Quota Analysis (Ultra Plan: 20,000 jobs/month):
  This run used: 450 jobs (2.3% of quota)
  Projected monthly: 450/day Ã— 30 = 13,500 jobs/month
  âœ“ Good! Projected 68% quota utilization
```

### Check Deduplication Savings

```
Query Analysis:
  Total query rows: 10
  Unique combinations: 6
  âœ… Quota saved: 4 API calls (40%)
```

### View Backfill Status

```bash
# Check which combinations are backfilled
psql $DATABASE_URL -c "SELECT title_keyword, location, jobs_found FROM backfill_tracking ORDER BY backfilled_date DESC LIMIT 10;"
```

---

## Troubleshooting

### Issue: Migrations already run

**Symptom:**
```
âœ“ Table already exists, columns already added
```

**Solution:** This is normal! Migrations are idempotent (safe to run multiple times)

---

### Issue: Backfill not triggering on CV upload

**Check:**
1. CV successfully parsed? (Check logs for "âœ“ Auto-generated N search query rows")
2. Queries created? (Check `user_search_queries` table)
3. API keys set? (Check `.env` file)

**Debug:**
```bash
# Check user queries
psql $DATABASE_URL -c "SELECT * FROM user_search_queries WHERE user_id = 1;"

# Check backfill tracking
psql $DATABASE_URL -c "SELECT * FROM backfill_tracking;"
```

---

### Issue: Daily cron not running

**Check:**
1. Is the service running? (Check logs)
2. Correct timezone? (Should be `Europe/Berlin`)
3. Next run time correct? (Check scheduler logs)

**Test manually:**
```bash
python scripts/daily_job_cron.py --run-once
```

---

## Quota Optimization Tips

1. **Reduce pages per query:**
   - Edit `src/jobs/user_backfill.py` â†’ Line 185: `num_pages=3` (instead of 5)
   - Edit `scripts/user_query_loader.py` â†’ Line 165: `num_pages=1` (already optimized)

2. **Filter low-priority combinations:**
   - Set `priority=0` for less important queries
   - Modify loader to skip priority < 5

3. **Increase daily update frequency:**
   - Run daily updates twice a day (6 AM + 6 PM)
   - Reduce backfill depth to 2 weeks instead of 1 month

4. **Monitor overlap:**
   - More users with similar interests = more savings
   - Track deduplication % in daily logs

---

## Next Steps

After setup:

1. âœ… **Test locally** with test users
2. âœ… **Deploy to Railway** (both web + cron services)
3. âœ… **Monitor quota usage** for first few days
4. âœ… **Adjust settings** if quota too high/low
5. âœ… **Add more cities** as users from new locations join

---

## Support Documents

- [NORMALIZED_QUERIES.md](./NORMALIZED_QUERIES.md) - How normalization works
- [BACKFILL_SYSTEM.md](./BACKFILL_SYSTEM.md) - Detailed backfill documentation
- [RAILWAY_DEPLOYMENT.md](./RAILWAY_DEPLOYMENT.md) - Deployment guide
- [API_FILTERS.md](./API_FILTERS.md) - API-level filtering guide

---

## Summary

You now have:
- âœ… Normalized query system for deduplication
- âœ… Backfill tracking to prevent duplicate fetches
- âœ… Automatic backfill on CV upload
- âœ… Daily cron job for fresh jobs
- âœ… Ready for Railway deployment

**Key benefit:** The more users with similar interests, the more quota you save!

First user: 100% quota used
Second user: ~50% quota used (50% overlap)
Third user: ~33% quota used (66% overlap)
...and so on!
